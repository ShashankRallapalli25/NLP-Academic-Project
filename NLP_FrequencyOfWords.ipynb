{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 3 - EAI 6010 70535 SEC 01 Fall 2023 CPS\n",
        "\n",
        "---\n",
        "**Student**: Venkata Satya Sai Shashank Rallapalli\n",
        "\n",
        "**Course**: EAI 6010 Applications of Artificial Intelligence\n",
        "\n",
        "**Term**: Fall 2023\n",
        "\n",
        "**Instructor**: Vladimir Shapiro\n",
        "\n",
        "**Date**: *10/07/2023*\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "<div style=\"page-break-after: always;\"></div>"
      ],
      "metadata": {
        "id": "j8ZhR498mvQd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Introduction:\n",
        "\n",
        "The goal of the artificial intelligence (AI) subfield known as \"natural language processing\" (NLP) is to make it possible for computers to comprehend, analyse, and produce human language. It enables machines to communicate with people in a meaningful and natural way. NLP is a broad field with a variety of uses, including chatbots and virtual assistants as well as sentiment analysis, language translation, and content summarization.\n",
        "\n",
        "The Natural Language Toolkit (NLTK) offers access to the large Gutenberg Corpus, which contains literary works like books, essays, and poems. It is a useful tool for many different NLP jobs (Gutenberg Corpus & Natural Language Generation | Meta-Guide.com, n.d.).\n"
      ],
      "metadata": {
        "id": "KavAUL8xm3mz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis:\n",
        "### Q1:\n",
        "### A. Download and install the Gutenberg corpus tool to your Jupyter Notebook. Project Gutenberg contains some 25,000 free electronic books hosted at http://www.gutenberg.org/. We can install the NLTK package and use the Gutenberg corpus."
      ],
      "metadata": {
        "id": "dJibOxhQOBrR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A1. A\n",
        "\n",
        "---\n",
        "An package called gutenbergr was created to make it easier to access the enormous library of public domain books that Project Gutenberg (http://www.gutenberg.org) makes available. A volunteer-run endeavour, Project Gutenberg provides a wide selection of free electronic books, including classic literature, historical works, and numerous other written works."
      ],
      "metadata": {
        "id": "oK7YQaBhtSfK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1 B. Download the Gutenberg corpus tool in the NLTK package"
      ],
      "metadata": {
        "id": "Oedf9-vsOkE5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###A1. B"
      ],
      "metadata": {
        "id": "cUjI-Ew4AeNE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "N5bEDwt-72O1",
        "outputId": "56fa3d28-3ee8-4e0e-f5ea-8fc5ba419630"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "#A1. B\n",
        "import nltk\n",
        "nltk.download('gutenberg')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. C. Use the texts in the corpus."
      ],
      "metadata": {
        "id": "6eK-VWk8OWV1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###A1. C"
      ],
      "metadata": {
        "id": "_wHHzc89Ah9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk import FreqDist\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Step 1: Download NLTK resources and Gutenberg corpus\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"gutenberg\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUy2L2Qdt-D1",
        "outputId": "be47eacb-11c5-40f5-85be-19d72985da10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. D. Create a table displaying relative (to the total number of modals in\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n",
        "each text) frequencies with which “modals” (can, could, may, might, will, would, and should) appear in each of the texts provided in the corpus"
      ],
      "metadata": {
        "id": "DcvdSti8PQKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A1. D"
      ],
      "metadata": {
        "id": "kGepY62SAmi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the list of modal verbs\n",
        "modal_verbs = [\"can\", \"could\", \"may\", \"might\", \"will\", \"would\", \"should\"]\n",
        "\n",
        "# Step 2: Calculate the relative frequencies of modals in each text\n",
        "modal_frequencies = []\n",
        "\n",
        "for file_id in gutenberg.fileids():\n",
        "    # Get the words in the text\n",
        "    words = gutenberg.words(file_id)\n",
        "\n",
        "    # Calculate the frequency of modal verbs in the text\n",
        "    total_modals = sum(1 for word in words if word.lower() in modal_verbs)\n",
        "    frequencies = [file_id] + [(words.count(modal) / total_modals)*100 for modal in modal_verbs]\n",
        "    modal_frequencies.append(frequencies)\n",
        "\n",
        "# Step 3: Display the table of relative frequencies using tabulate\n",
        "headers = [\"Text\"] + modal_verbs\n",
        "table = tabulate(modal_frequencies, headers, tablefmt=\"grid\")\n",
        "title = \"Table 1. Relative Frequencies of Modal Verbs in Texts\"\n",
        "print(title)\n",
        "print(table)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "mPapd3j6qiXV",
        "outputId": "811bdc1a-88ed-49ac-e70f-9775a98242c8"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Table 1. Relative Frequencies of Modal Verbs in Texts\n",
            "+-------------------------+----------+----------+----------+----------+----------+----------+----------+\n",
            "| Text                    |      can |    could |      may |    might |     will |    would |   should |\n",
            "+=========================+==========+==========+==========+==========+==========+==========+==========+\n",
            "| austen-emma.txt         |  7.87861 | 24.0735  |  6.21535 |  9.39597 | 16.3116  | 23.7817  | 10.6799  |\n",
            "+-------------------------+----------+----------+----------+----------+----------+----------+----------+\n",
            "| austen-persuasion.txt   |  6.57462 | 29.1913  |  5.71992 | 10.9139  | 10.6509  | 23.0769  | 12.1631  |\n",
            "+-------------------------+----------+----------+----------+----------+----------+----------+----------+\n",
            "| austen-sense.txt        |  8.95652 | 24.6957  |  7.34783 |  9.34783 | 15.3913  | 22.0435  |  9.91304 |\n",
            "+-------------------------+----------+----------+----------+----------+----------+----------+----------+\n",
            "| bible-kjv.txt           |  3.05464 |  2.36627 | 14.6852  |  6.81199 | 54.5963  |  6.35308 | 11.0139  |\n",
            "+-------------------------+----------+----------+----------+----------+----------+----------+----------+\n",
            "| blake-poems.txt         | 35.7143  |  5.35714 |  8.92857 |  3.57143 |  5.35714 |  5.35714 | 10.7143  |\n",
            "+-------------------------+----------+----------+----------+----------+----------+----------+----------+\n",
            "| bryant-stories.txt      | 12.9758  | 26.6436  |  3.11419 |  3.97924 | 24.9135  | 19.0311  |  6.57439 |\n",
            "+-------------------------+----------+----------+----------+----------+----------+----------+----------+\n",
            "| burgess-busterbrown.txt | 12.8492  | 31.2849  |  1.67598 |  9.49721 | 10.6145  | 25.6983  |  7.26257 |\n",
            "+-------------------------+----------+----------+----------+----------+----------+----------+----------+\n",
            "| carroll-alice.txt       | 17.5926  | 22.5309  |  3.39506 |  8.64198 |  7.40741 | 21.6049  |  8.33333 |\n",
            "+-------------------------+----------+----------+----------+----------+----------+----------+----------+\n",
            "| chesterton-ball.txt     | 15.4846  | 13.8298  | 10.6383  |  8.15603 | 23.4043  | 16.4303  |  8.86525 |\n",
            "+-------------------------+----------+----------+----------+----------+----------+----------+----------+\n",
            "| chesterton-brown.txt    | 17.3077  | 23.3516  |  6.45604 |  9.75275 | 15.2473  | 18.1319  |  7.69231 |\n",
            "+-------------------------+----------+----------+----------+----------+----------+----------+----------+\n",
            "| chesterton-thursday.txt | 16.6667  | 21.0826  |  7.97721 | 10.114   | 15.5271  | 16.5242  |  7.69231 |\n",
            "+-------------------------+----------+----------+----------+----------+----------+----------+----------+\n",
            "| edgeworth-parents.txt   | 13.9287  | 17.2061  |  6.55469 |  5.20279 | 21.1798  | 20.6063  | 11.102   |\n",
            "+-------------------------+----------+----------+----------+----------+----------+----------+----------+\n",
            "| melville-moby_dick.txt  | 11.6959  | 11.4301  | 12.2275  |  9.72887 | 20.1489  | 22.3817  |  9.62254 |\n",
            "+-------------------------+----------+----------+----------+----------+----------+----------+----------+\n",
            "| milton-paradise.txt     | 14.5578  |  8.43537 | 15.7823  | 13.3333  | 21.9048  |  6.66667 |  7.48299 |\n",
            "+-------------------------+----------+----------+----------+----------+----------+----------+----------+\n",
            "| shakespeare-caesar.txt  |  4.74777 |  5.34125 | 10.3858  |  3.56083 | 38.2789  | 11.8694  | 11.276   |\n",
            "+-------------------------+----------+----------+----------+----------+----------+----------+----------+\n",
            "| shakespeare-hamlet.txt  |  7.51708 |  5.92255 | 12.7563  |  6.37813 | 29.8405  | 13.6674  | 11.8451  |\n",
            "+-------------------------+----------+----------+----------+----------+----------+----------+----------+\n",
            "| shakespeare-macbeth.txt |  8.33333 |  5.95238 | 11.9048  |  1.98413 | 24.6032  | 16.6667  | 16.2698  |\n",
            "+-------------------------+----------+----------+----------+----------+----------+----------+----------+\n",
            "| whitman-leaves.txt      | 12.9794  |  7.22714 | 12.5369  |  3.83481 | 38.4956  | 12.5369  |  6.19469 |\n",
            "+-------------------------+----------+----------+----------+----------+----------+----------+----------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. E.\n",
        "### I. Determine two modals with the largest span of relative (to the total number of modals in each text) frequencies (most used minus least used)."
      ],
      "metadata": {
        "id": "xwS0F0_VP_VM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A1. E. I"
      ],
      "metadata": {
        "id": "DhFLu-v0A-pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Determine the two modals with the largest span of relative frequencies\n",
        "max_span = 0\n",
        "selected_modals = []\n",
        "\n",
        "for modal1 in modal_verbs:\n",
        "    for modal2 in modal_verbs:\n",
        "        if modal1 != modal2:\n",
        "            modal1_freqs = [f[modal_verbs.index(modal1) + 1] for f in modal_frequencies]\n",
        "            modal2_freqs = [f[modal_verbs.index(modal2) + 1] for f in modal_frequencies]\n",
        "            span = max(modal1_freqs) - min(modal1_freqs) + max(modal2_freqs) - min(modal2_freqs)\n",
        "            if span > max_span:\n",
        "                max_span = span\n",
        "                selected_modals = [modal1, modal2]\n",
        "print(selected_modals)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0npmaFZ7uP2L",
        "outputId": "bc372807-09b8-4e6f-a949-713806811bac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['will', 'can']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. E. II. For the most frequently used modal among the two modals with the largest span, select a text that uses it the most and the one that uses it the least."
      ],
      "metadata": {
        "id": "B-I_n6k1QBB5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A1. E. II"
      ],
      "metadata": {
        "id": "6A-FlDJsBFvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Find the text that uses the most and least frequent modal among the two\n",
        "\n",
        "max_modal = \"\"\n",
        "max_count = 0\n",
        "\n",
        "# Iterate through each modal in the selected_modals list\n",
        "for modal in selected_modals:\n",
        "    total_count = 0\n",
        "\n",
        "    # Iterate through each text in the Gutenberg corpus\n",
        "    for file_id in gutenberg.fileids():\n",
        "        words = gutenberg.words(file_id)\n",
        "        total_count += words.count(modal)\n",
        "\n",
        "    # Check if the current modal has a higher total count than the previous maximum\n",
        "    if total_count > max_count:\n",
        "        max_count = total_count\n",
        "        max_modal = modal\n",
        "\n",
        "# Print the modal with the maximum total count\n",
        "print(\"Modal with Maximum Total Count:\", max_modal)\n",
        "print(\"Total Count:\", max_count)\n",
        "#This code will calculate the total count of each modal verb across all texts in the Gutenberg corpus and then identify and print the modal with the maximum total count.\n",
        "\n",
        "\n",
        "most_frequent_text = None\n",
        "least_frequent_text = []\n",
        "max_frequency = 0\n",
        "min_frequency = float(\"inf\")\n",
        "\n",
        "for file_id in gutenberg.fileids():\n",
        "    words = gutenberg.words(file_id)\n",
        "    modal_frequency = words.count(max_modal)\n",
        "\n",
        "    if modal_frequency > max_frequency:\n",
        "        max_frequency = modal_frequency\n",
        "        most_frequent_text = file_id\n",
        "\n",
        "for file_id in gutenberg.fileids():\n",
        "    words = gutenberg.words(file_id)\n",
        "    modal_frequency = words.count(max_modal)\n",
        "    if modal_frequency < min_frequency:\n",
        "        min_frequency = modal_frequency\n",
        "        least_frequent_text = [file_id]\n",
        "\n",
        "print(f\"The text that uses '{max_modal}' '{ max_frequency}' the most is '{most_frequent_text}'\")\n",
        "print(f\"The text that uses '{max_modal}' '{min_frequency}'the least is '{least_frequent_text}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cawxC0hcutga",
        "outputId": "811e8bfc-9941-4fd1-eb4a-a6e4045b76c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modal with Maximum Total Count: will\n",
            "Total Count: 7130\n",
            "The text that uses 'will' '3807' the most is 'bible-kjv.txt'\n",
            "The text that uses 'will' '3'the least is '['blake-poems.txt']'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. E. III. For the most frequently used modal among the two with the largest span, try to explain why this word is used differently in the two texts."
      ],
      "metadata": {
        "id": "uYauFdJ-QI61"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A1 E III.\n",
        "The different content and writing styles of the two texts, \"bible-kjv.txt\" and \"blake-poems.txt,\" can be used to explain the discrepancy in the word counts of \"will\" between them.\n",
        "\n",
        "'bible-kjv.txt': The King James Version of the Bible is the religious text being discussed; it is made up of religious stories, lessons, and historical facts. In this context, the word \"will\" is primarily used to convey divine will, prophesy, or God's plans. For instance, the phrase \"Thy will be done\" is frequently used in religious contexts. Therefore, the term \"will\" frequently appears in the Bible in these doctrinal and prophetic settings, which may lead to a significantly larger word count for \"will.\"\n",
        "\n",
        "'blake-poems.txt': This document comprises poems written by renowned poet and artist William Blake. Depending on the particular poem and its themes, the word \"will\" in poetry can have a variety of meanings and connotations. It might be employed to signify individual will, desire, and tenacity, or even as a poetic device. The word \"will\" has a wider range of meanings in poetry than it does in other situations, including those that are not religious or prophetic. As a result, the frequency and usage pattern of the word \"will\" in \"blake-poems.txt\" may differ from how it is used in the Bible.\n",
        "\n",
        "In conclusion, the disparity in word counts for the word \"will\" between these two books shows their distinct themes and content as well as the various ways the word is used.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DTo5r1dk6RSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the text 'bible-kjv.txt'\n",
        "bible_text = gutenberg.raw('blake-poems.txt')\n",
        "print(bible_text)\n"
      ],
      "metadata": {
        "id": "C8JAebJS4sr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q2:\n",
        "###A. In the Inaugural corpus"
      ],
      "metadata": {
        "id": "_JjSpvN6vVXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A2. A"
      ],
      "metadata": {
        "id": "FbhDj2HOBtAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('inaugural')\n",
        "from nltk.corpus import inaugural"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hn9GWv_vZP-",
        "outputId": "65d17a2b-cd2e-4d4c-edaa-b63df388f91d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/inaugural.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2. B. Chose Kennedy’s speech,"
      ],
      "metadata": {
        "id": "jLcTCPztvqVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###A2. B"
      ],
      "metadata": {
        "id": "r8-t-MK6BxWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "from collections import Counter\n",
        "\n",
        "# Load Kennedy's inaugural speech\n",
        "kennedy_speech = inaugural.words('1961-Kennedy.txt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73SCzduc6fta",
        "outputId": "71f6e493-0d1d-4565-e79e-8fc12b272a5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2. C. Identify the 10 most frequently used long words (words longer than 7 characters)."
      ],
      "metadata": {
        "id": "36Rm79pAv4Q3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A2. C"
      ],
      "metadata": {
        "id": "oF84IWcSB1gI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter long words (words longer than 7 characters)\n",
        "long_words = [word.lower() for word in kennedy_speech if len(word) > 7]\n",
        "\n",
        "# Count word frequencies\n",
        "word_freq = Counter(long_words)\n",
        "\n",
        "# Get the 10 most frequent long words\n",
        "most_common_long_words = word_freq.most_common(10)\n",
        "# Print the 10 most frequent long words\n",
        "print(\"10 Most Frequent Long Words:\")\n",
        "for word, freq in most_common_long_words:\n",
        "    print(f\"{word}: {freq} times\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e0sW3CCv4wJ",
        "outputId": "b8b7a32f-4d94-448d-9643-6adddfc9892a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 Most Frequent Long Words:\n",
            "citizens: 5 times\n",
            "president: 4 times\n",
            "americans: 4 times\n",
            "generation: 3 times\n",
            "forebears: 2 times\n",
            "revolution: 2 times\n",
            "committed: 2 times\n",
            "powerful: 2 times\n",
            "supporting: 2 times\n",
            "themselves: 2 times\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Q2. D. Which one of those 10 words has the largest number of synonyms? Print out the number. Use WordNet as a helper"
      ],
      "metadata": {
        "id": "a2MqIOiov-hA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###A2. D."
      ],
      "metadata": {
        "id": "SwG82dHKB6ww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the word with the largest number of synonyms using WordNet\n",
        "max_synonym_count = 0\n",
        "word_with_max_synonyms = None\n",
        "\n",
        "for word, _ in most_common_long_words:\n",
        "    #print(f\"\\nWord: {word}\")\n",
        "    # Get WordNet synsets for the word\n",
        "    synsets = wordnet.synsets(word)\n",
        "\n",
        "    synonyms = (set(synonym for synset in synsets for synonym in synset.lemma_names()))\n",
        "    # Count the number of synonyms for the word\n",
        "    synonym_count = len(set(synonym for synset in synsets for synonym in synset.lemma_names()))\n",
        "    #print(\"Synonyms:\")\n",
        "    #print(\", \".join(synonyms))\n",
        "    # Update the max_synonym_count and word_with_max_synonyms if needed\n",
        "    if synonym_count > max_synonym_count:\n",
        "        max_synonym_count = synonym_count\n",
        "        word_with_max_synonyms = word\n",
        "# Print the word with the largest number of synonyms\n",
        "print(\"\\nWord with the Largest Number of Synonyms:\")\n",
        "print(f\"{word_with_max_synonyms}: {max_synonym_count} synonyms\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-82ruB4Fwoiy",
        "outputId": "d10abd9e-d28f-49b3-848a-4404a7a50929"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word with the Largest Number of Synonyms:\n",
            "supporting: 37 synonyms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q2. E.\n",
        "###I. List all synonyms for the 10 most frequently used words (words longer than 7 characters)."
      ],
      "metadata": {
        "id": "JJiEHrQbwFNp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A2. E. I"
      ],
      "metadata": {
        "id": "C3Y1TAv3CCeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for word, _ in most_common_long_words:\n",
        "    print(f\"\\nWord: {word}\")\n",
        "    # Get WordNet synsets for the word\n",
        "    synsets = wordnet.synsets(word)\n",
        "\n",
        "    synonyms = (set(synonym for synset in synsets for synonym in synset.lemma_names()))\n",
        "    # Count the number of synonyms for the word\n",
        "    synonym_count = len(set(synonym for synset in synsets for synonym in synset.lemma_names()))\n",
        "    print(\"Synonyms:\")\n",
        "    print(\", \".join(synonyms))\n",
        "    # Update the max_synonym_count and word_with_max_synonyms if needed\n",
        "    if synonym_count > max_synonym_count:\n",
        "        max_synonym_count = synonym_count\n",
        "        word_with_max_synonyms = word"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlT5VnugwyGz",
        "outputId": "9b941526-12cf-4b64-aa6f-7efa2aea53a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word: citizens\n",
            "Synonyms:\n",
            "citizen\n",
            "\n",
            "Word: president\n",
            "Synonyms:\n",
            "chairwoman, President_of_the_United_States, chairperson, prexy, President, United_States_President, president, chair, Chief_Executive, chairman\n",
            "\n",
            "Word: americans\n",
            "Synonyms:\n",
            "American_English, American, American_language\n",
            "\n",
            "Word: generation\n",
            "Synonyms:\n",
            "genesis, coevals, generation, contemporaries, multiplication, propagation\n",
            "\n",
            "Word: forebears\n",
            "Synonyms:\n",
            "forbear, forebear\n",
            "\n",
            "Word: revolution\n",
            "Synonyms:\n",
            "revolution, gyration, rotation\n",
            "\n",
            "Word: committed\n",
            "Synonyms:\n",
            "give, institutionalise, committed, charge, put, entrust, commit, practice, send, consecrate, trust, pull, attached, institutionalize, devote, confide, place, perpetrate, dedicate, invest, intrust\n",
            "\n",
            "Word: powerful\n",
            "Synonyms:\n",
            "mighty, potent, muscular, herculean, knock-down, powerful, brawny, right, sinewy, mightily, hefty\n",
            "\n",
            "Word: supporting\n",
            "Synonyms:\n",
            "underpin, suffer, corroborate, digest, back_up, patronize, back, patronage, supporting, plump_for, affirm, endorse, stand, patronise, defend, brook, load-bearing, encouraging, stomach, subscribe, support, bear, abide, fend_for, confirm, sustain, plunk_for, substantiate, endure, tolerate, hold_up, keep_going, bear_out, indorse, hold, put_up, stick_out\n",
            "\n",
            "Word: themselves\n",
            "Synonyms:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2. E. II. Which one of those 10 words has the largest number of hyponyms? Print out the number"
      ],
      "metadata": {
        "id": "SxdQRkTPwMmb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A2. E. II"
      ],
      "metadata": {
        "id": "ZB3ZJdjLCONz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import count\n",
        "# Initialize variables to keep track of the word with the most hyponyms and its count\n",
        "word_with_max_hyponyms = None\n",
        "max_hyponyms = 0\n",
        "\n",
        "# Find hyponyms using WordNet for each of the 10 most frequent long words\n",
        "for word, _ in most_common_long_words:\n",
        "    print(f\"Word: {word}\")\n",
        "\n",
        "    # Find hyponyms using WordNet\n",
        "    synsets = wordnet.synsets(word)\n",
        "    hyponyms_count = {}\n",
        "    for synset in synsets:\n",
        "        hyponyms = synset.hyponyms()\n",
        "        if hyponyms:\n",
        "            for hyponym in hyponyms:\n",
        "                #print(f\"- {hyponym.lemmas()[0].name()}\")\n",
        "                hyponyms_count[synset.name()] = len(hyponym.hyponyms())\n",
        "    print(hyponyms_count)\n",
        "    if hyponyms_count:\n",
        "        # Find the word with the largest number of hyponyms\n",
        "        max_word_hyponyms = max(hyponyms_count, key=hyponyms_count.get)\n",
        "        num_hyponyms = hyponyms_count[max_word_hyponyms]\n",
        "\n",
        "        print(f\"Number of Hyponyms: {num_hyponyms}\")\n",
        "\n",
        "        # Check if this word has the largest number of hyponyms\n",
        "        if num_hyponyms > max_hyponyms:\n",
        "            max_hyponyms = num_hyponyms\n",
        "            word_with_max_hyponyms = word\n",
        "        #print()\n",
        "    else:\n",
        "        #print(\"No hyponyms found for this word.\")\n",
        "        print()\n",
        "\n",
        "# Print the word with the largest number of hyponyms\n",
        "if word_with_max_hyponyms is not None:\n",
        "    print(f\"Word with the Largest Number of Hyponyms: {word_with_max_hyponyms} ({max_hyponyms} hyponyms)\")\n",
        "else:\n",
        "    print(\"No word with hyponyms found among the 10 most frequent long words.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iE4xbfFrxGbf",
        "outputId": "cd666491-6b22-46b3-a756-8fb3e53f98fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: citizens\n",
            "{'citizen.n.01': 4}\n",
            "Number of Hyponyms: 4\n",
            "Word: president\n",
            "{'president.n.03': 0, 'president.n.04': 0}\n",
            "Number of Hyponyms: 0\n",
            "Word: americans\n",
            "{'american.n.01': 0, 'american_english.n.01': 0, 'american.n.03': 11}\n",
            "Number of Hyponyms: 11\n",
            "Word: generation\n",
            "{'coevals.n.01': 2, 'generation.n.02': 0, 'generation.n.07': 0}\n",
            "Number of Hyponyms: 2\n",
            "Word: forebears\n",
            "{'forebear.n.01': 2}\n",
            "Number of Hyponyms: 2\n",
            "Word: revolution\n",
            "{'revolution.n.01': 0, 'revolution.n.02': 0, 'rotation.n.03': 3}\n",
            "Number of Hyponyms: 3\n",
            "Word: committed\n",
            "{'perpetrate.v.01': 0, 'give.v.18': 0, 'commit.v.03': 0, 'entrust.v.01': 0, 'invest.v.01': 0}\n",
            "Number of Hyponyms: 0\n",
            "Word: powerful\n",
            "{}\n",
            "\n",
            "Word: supporting\n",
            "{'support.n.08': 0, 'support.v.01': 0, 'support.v.02': 0, 'back.v.01': 2, 'hold.v.10': 0, 'confirm.v.01': 0, 'defend.v.01': 0, 'digest.v.03': 0}\n",
            "Number of Hyponyms: 2\n",
            "Word: themselves\n",
            "{}\n",
            "\n",
            "Word with the Largest Number of Hyponyms: americans (11 hyponyms)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q2. F. List all hyponyms of the 10 most frequently used words (words longer than 7 characters)."
      ],
      "metadata": {
        "id": "NBQP30fZwSDr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###A2. F"
      ],
      "metadata": {
        "id": "ZRUVHkJRCTgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import inaugural, wordnet\n",
        "from collections import Counter\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Load Kennedy's inaugural speech\n",
        "kennedy_speech = inaugural.words('1961-Kennedy.txt')\n",
        "\n",
        "# Filter long words (words longer than 7 characters)\n",
        "long_words = [word.lower() for word in kennedy_speech if len(word) > 7]\n",
        "\n",
        "# Count word frequencies\n",
        "word_freq = Counter(long_words)\n",
        "\n",
        "# Get the 10 most frequent long words\n",
        "most_common_long_words = word_freq.most_common(10)\n",
        "\n",
        "# Initialize variables to keep track of the word with the most hyponyms and its count\n",
        "word_with_max_hyponyms = None\n",
        "max_hyponyms = 0\n",
        "\n",
        "# Create an HTML table to display hyponyms\n",
        "hyponyms_table = \"<table style='border-collapse: collapse; width: 70%;'>\"\n",
        "hyponyms_table += \"<tr><th>Word</th><th>Hyponyms</th></tr>\"\n",
        "\n",
        "# Find hyponyms using WordNet for each of the 10 most frequent long words\n",
        "for word, _ in most_common_long_words:\n",
        "\n",
        "    # Find hyponyms using WordNet\n",
        "    synsets = wordnet.synsets(word)\n",
        "    hyponyms_count = {}\n",
        "    hyponyms_list = []\n",
        "\n",
        "    for synset in synsets:\n",
        "        hyponyms = synset.hyponyms()\n",
        "        if hyponyms:\n",
        "            for hyponym in hyponyms:\n",
        "                hyponyms_count[synset.name()] = len(hyponym.hyponyms())\n",
        "                hyponyms_list.extend([lemma.name() for lemma in hyponym.lemmas()])\n",
        "\n",
        "    if hyponyms_count:\n",
        "\n",
        "\n",
        "        hyponyms_table += f\"<tr><td>{word}</td><td>{', '.join(hyponyms_list)}</td></tr>\"\n",
        "\n",
        "        # Check if this word has the largest number of hyponyms\n",
        "        if num_hyponyms > max_hyponyms:\n",
        "            max_hyponyms = num_hyponyms\n",
        "            word_with_max_hyponyms = word\n",
        "\n",
        "# Close the HTML table\n",
        "hyponyms_table += \"</table>\"\n",
        "\n",
        "# Display the HTML table\n",
        "HTML(hyponyms_table)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "ZfwdNUIVEK4C",
        "outputId": "eab00f91-99da-4354-c648-d29da5c5c16a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table style='border-collapse: collapse; width: 70%;'><tr><th>Word</th><th>Hyponyms</th></tr><tr><td>citizens</td><td>active_citizen, civilian, freeman, freewoman, private_citizen, repatriate, thane, voter, elector</td></tr><tr><td>president</td><td>ex-president, Kalon_Tripa, vice_chairman</td></tr><tr><td>americans</td><td>African-American, African_American, Afro-American, Black_American, Alabaman, Alabamian, Alaskan, Anglo-American, Appalachian, Arizonan, Arizonian, Arkansan, Arkansawyer, Asian_American, Bay_Stater, Bostonian, Californian, Carolinian, Coloradan, Connecticuter, Creole, Delawarean, Delawarian, Floridian, Franco-American, Georgian, German_American, Hawaiian, Idahoan, Illinoisan, Indianan, Hoosier, Iowan, Kansan, Kentuckian, Bluegrass_Stater, Louisianan, Louisianian, Mainer, Down_Easter, Marylander, Michigander, Wolverine, Minnesotan, Gopher, Mississippian, Missourian, Montanan, Nebraskan, Cornhusker, Nevadan, New_Englander, Yankee, New_Hampshirite, Granite_Stater, New_Jerseyan, New_Jerseyite, Garden_Stater, New_Mexican, New_Yorker, Nisei, North_Carolinian, Tarheel, North_Dakotan, Ohioan, Buckeye, Oklahoman, Sooner, Oregonian, Beaver, Pennsylvanian, Keystone_Stater, Puerto_Rican, Rhode_Islander, South_Carolinian, South_Dakotan, Southerner, Spanish_American, Hispanic_American, Hispanic, Tennessean, Volunteer, Texan, Tory, Utahan, Vermonter, Virginian, Washingtonian, Washingtonian, West_Virginian, Wisconsinite, Badger, Wyomingite, Yankee, Yank, Northerner, Yankee, Yank, Yankee-Doodle, African_American_Vernacular_English, AAVE, African_American_English, Black_English, Black_English_Vernacular, Black_Vernacular, Black_Vernacular_English, Ebonics, Creole, Latin_American, Latino, Mesoamerican, North_American, South_American, West_Indian</td></tr><tr><td>generation</td><td>peer_group, youth_culture, baby_boom, baby-boom_generation, generation_X, gen_X, posterity, biogenesis, biogeny</td></tr><tr><td>forebears</td><td>grandparent, great_grandparent</td></tr><tr><td>revolution</td><td>Cultural_Revolution, Great_Proletarian_Cultural_Revolution, green_revolution, counterrevolution, axial_rotation, axial_motion, roll, dextrorotation, clockwise_rotation, levorotation, counterclockwise_rotation, orbital_rotation, orbital_motion, spin</td></tr><tr><td>committed</td><td>make, recommit, apply, rededicate, vow, consecrate, hospitalize, hospitalise, commend, consign, charge, obligate, recommit, buy_into, fund, roll_over, shelter, speculate, job, tie_up</td></tr><tr><td>supporting</td><td>shoring, shoring_up, propping_up, suspension, dangling, hanging, help, assist, aid, patronize, patronise, shop, shop_at, buy_at, frequent, sponsor, promote, advance, boost, further, encourage, second, back, endorse, indorse, sponsor, undergird, fund, provide, bring_home_the_bacon, see_through, sponsor, patronize, patronise, subsidize, subsidise, champion, defend, guarantee, warrant, block, brace, bracket, buoy, buoy_up, carry, chock, pole, prop_up, prop, shore_up, shore, scaffold, truss, underpin, back, back_up, document, prove, demonstrate, establish, show, shew, validate, verify, vouch, apologize, apologise, excuse, justify, rationalize, rationalise, stand_up, stick_up, uphold, accept, live_with, swallow, bear_up, pay, sit_out, stand_for, hold_still_for, take_a_joke, take_lying_down</td></tr></table>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q2. G. Reflect on the results."
      ],
      "metadata": {
        "id": "5lA7-F9lwVfS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###A2. G\n",
        "The number of hyponyms indicated for each word is based on the unique hyponyms identified across all senses of that word, whereas the count in the given code represents the total number of hyponyms found for a specific word sense (synset) in WordNet.\n",
        "\n",
        "To summarize, the shown hyponyms reveal all unique hyponyms discovered for that word, including those shared between senses, whereas the count reflects the overall number of hyponyms across all senses of a word. As a result, the count and the number of hyponyms that are displayed for each term may differ."
      ],
      "metadata": {
        "id": "-fuM9aV557nc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q2. H. Run all the cells, ensure all are executed, and create the output. It’s your choice whether to execute locally or on Google Colab"
      ],
      "metadata": {
        "id": "vIqTP-2twanj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###A2. H\n",
        "All the cells are executed in the google colab. Output will be converted into pdf format and submitted for readability purpose."
      ],
      "metadata": {
        "id": "5L6CjtKk6VqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conclusion:\n",
        "\n",
        "In conclusion, the study of relative modal frequencies provided by the code provides important information about the use of modal verbs in various texts. The linguistic study, text analysis, and comprehension of the subtle differences in writing styles across different literary works can all benefit from these discoveries.\n",
        "\n",
        "The above code looks up synonyms and hyponyms for the ten most often used big words (words longer than 7 characters) in John F. Kennedy's inauguration speech. The code offers insightful information about the semantic connections between words in Kennedy's inauguration address. It illustrates the variety of synonyms as well as the various levels of specificity (hyponyms) connected to various words. Understanding the language employed in the speech and investigating the subtleties of word meanings can both benefit from this research."
      ],
      "metadata": {
        "id": "nhrqRiOi7zr_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References:\n",
        "\n",
        "\n",
        "\n",
        "*   Sam, J. (2021, December 14). Working with Corpus in NLTK — PART-2 - Jeffy Sam - Medium. Medium. https://medium.com/@jeffysam02/working-with-corpus-in-nltk-part-2-2afbc5fe1457\n",
        "*   Johnson, D. (2023, September 30). NLTK WordNet: Find Synonyms from NLTK WordNet in Python. Guru99. https://www.guru99.com/wordnet-nltk.htm\n",
        "*   Gutenberg Corpus & Natural Language Generation | Meta-Guide.com. (n.d.). https://meta-guide.com/natural-language/nlp/nlg/gutenberg-corpus-natural-language-generation\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vSJA_W3p6qy6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bATLxEi07FRU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}